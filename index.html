<!DOCTYPE html>
<html>
  <!-- Fix for favicon import error -->
  <link rel="shortcut icon" href="#" />
  <head>
    <!-- MVP.css drop-in classless minimalist stylesheet -->
    <link rel="stylesheet" href="https://unpkg.com/mvp.css" />
    <style>
      table {
        font-family: arial, sans-serif;
        border-collapse: collapse;
        width: 100%;
      }

      td,
      th {
        border: 1px solid #dddddd;
        text-align: left;
        padding: 8px;
      }

      tr:nth-child(even) {
        background-color: #dddddd;
      }

      .top-level-container {
        width: 80vw;
        height: 100vh;
        padding: 5vh 5vw;
      }
      .study-button {
        /* CSS styles for this button taken from https://copy-paste-css.com/ */
        display: inline-block;
        outline: 0;
        cursor: pointer;
        border: none;
        padding: 0 56px;
        height: 45px;
        line-height: 45px;
        border-radius: 7px;
        background-color: #0070f3;
        color: white;
        font-weight: 400;
        font-size: 16px;
        box-shadow: 0 4px 14px 0 rgb(0 118 255 / 39%);
        transition: background 0.2s ease, color 0.2s ease, box-shadow 0.2s ease;
        :hover {
          background: rgba(0, 118, 255, 0.9);
          box-shadow: 0 6px 20px rgb(0 118 255 / 23%);
        }
      }
      a {
        text-decoration: none;
      }
    </style>
    <title>
      Comparative Analysis of Accurately Identifying the Median Across Different
      Graph Types
    </title>
  </head>
  <body>
    <h1 style="text-align: center; font-size: 2.5em; margin-top: 0.5em">
      Comparative Analysis of Accurately Identifying the Median Across Different
      Graph Types
    </h1>
    <h2 style="text-align: center; font-size: 1.5em; margin-top: 0.5em">
      Project Group Members: Axel Luca, Mahir Sowad, Jackson Martinez Balcazar,
      Humza Qureshi
    </h2>
    <div style="display: flex; justify-content: center">
      <iframe
        width="560"
        height="315"
        src="https://www.youtube.com/embed/Fh0a2KNuJy4?si=UxQJO9GyHZT-Hq7b"
        title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
      ></iframe>
    </div>

    <div class="top-level-container">
      <h1>Results Overview</h1>
      <p>
        This project extends the component of our A3 experiment where we
        evaluate how accurately people perceive the median of the underlying
        dataset of a histogram. For this experiment, we have 5 different types
        of visualizations: histograms, density plots, scatter plots, beeswarm
        plots, and bar charts. The overarching goal of our experiment here was
        to determine how different graphs may impact users' ability to
        accurately find the median of the underlying dataset being visualized.
        It should be noted that for the sake of simplifying both our experiment
        and the task presented to users, if the elements of datasets we made
        both had x-coordinate values and y-coordinate values, the users only had
        to find the median of the x-coordinate values of the dataset.
      </p>
      <h1>Visualizations Tested</h1>
      <p>This experiment tested the following visualization types:</p>
      <ul>
        <li><strong>Histograms</strong></li>
        <li><strong>Density Plots</strong></li>
        <li><strong>Scatter Plots</strong></li>
        <li><strong>Beeswarm Plots</strong></li>
        <li><strong>Bar Charts</strong></li>
      </ul>
      <div>
        <h1>Process Book</h1>
        <p>Our process book can be found here:</p>
        <iframe
          style="width: 100%; background-color: #ffffff; min-height: 500px"
          src="https://docs.google.com/document/d/e/2PACX-1vTZiNqBpdFNpeJGXxk2PzJyNIHLlUCbMOG3koZ16vqfXZ1bgVI2zL3zkqiVZsUgCu7dh3JeZ_eIebjo/pub?embedded=true"
        ></iframe>
        <a
          class="study-button"
          href="https://docs.google.com/document/d/e/2PACX-1vTZiNqBpdFNpeJGXxk2PzJyNIHLlUCbMOG3koZ16vqfXZ1bgVI2zL3zkqiVZsUgCu7dh3JeZ_eIebjo/pub"
          target="_blank"
        >
          View the Process Book in a New Tab
        </a>
      </div>
      <h1>Study made using ReVISit</h1>
      <a
        class="study-button"
        href="https://mahirsowad3.github.io/CS573-grad-final-project/"
        target="_blank"
      >
        See our study here
      </a>
      <div>
        <h1>Generated Datasets</h1>
        <p>The data used to create all of our graphs can be found here:</p>
        <a
          class="study-button"
          href="https://drive.google.com/drive/folders/1punbHk2eShji9NLohtZ4EaVlOxXegPw3?usp=drive_link"
          target="_blank"
        >
          Check out the datasets used to create all of our graphs here
        </a>
      </div>
      <div>
        <h1>Participant Data Collected</h1>
        <p>
          Due to the confidentiality reasons mentioned in our study's consent
          page, we could not publish our participants' experiment data.
        </p>
      </div>
      <h1>Results Summary</h1>
      <p>
        The table below shows the average error calculations for each of our
        visualizations. For all of the different charts, the original error
        metric calculated was the absolute difference between the true median
        x-coordinate value of the graphs' underlying dataset and the
        user-selected x-coordinate value of the graphs' underlying dataset.
        However, during the error calculation, we ran into difficulties
        comparing the absolute errors of the raw values of the graphs. This is
        due to the fact that during the data generation process for the
        underlying datasets, different ranges / scales across different graph
        types were utilized to create the most appropriate datasets for each
        graph type. This made comparison of the raw absolute errors between
        graph types difficult because the absolute error for one graph type
        could be greater in terms of magnitude than the absolute error of a
        different graph type, but in terms of percentage error, the error may
        not be as significant as the raw magnitude would have one believe. To
        address this issue, we normalized all of the obtained error calculations
        throughout our experiment by dividing all of them by the difference
        between the maximum and minimum x-coordinate values of their
        corresponding graphs' underlying datasets. Therefore, the final formula
        used for calculating the error calculations throughout the entire
        experiment was:
      </p>
      <img
        src="error_calculation_formula.png"
        alt="Formula for Error Calculation"
        width="1200"
      />
      <p>
        Once all of the normalized error calculations were calculated, we then
        grouped them all by their corresponding vsiaulization. After doing so,
        we were able to calculate the average error for each visualization by
        summing up all their corresponding normalized errors and then dividing
        the total by the number of trials conducted for that particular
        visualization. The formula representing this calculation for each
        visualization is:
      </p>
      <img
        src="average_error_calculation.png"
        alt="Formula for Average Error Calculation"
        width="500"
      />
      <h2>Average Error For Each Visualization:</h2>
      <table>
        <tr>
          <th>Visualization</th>
          <th>Error</th>
        </tr>
        <tr>
          <td>Beeswarm Plot</td>
          <td>0.0199</td>
        </tr>
        <tr>
          <td>Density Plot</td>
          <td>0.0289</td>
        </tr>
        <tr>
          <td>Histogram</td>
          <td>0.0359</td>
        </tr>
        <tr>
          <td>Bar Chart</td>
          <td>0.0528</td>
        </tr>
        <tr>
          <td>Scatter Plot</td>
          <td>0.0732</td>
        </tr>
      </table>
      <p>
        An interesting result that came out of these calculations are that
        despite its unique format, the bar chart visualization did not have the
        largest average error calculation. A possible reason why this is the
        case is the fact that we gave hints to the users in order to help them
        figure out how to interpret this visualization correctly.
      </p>
      <h1>95% Confidence Intervals For Errors</h1>
      <img
        src="confidence_intervals.png"
        alt="Confidence Intervals"
        height="500"
        width="1000"
      />
      <p>
        From this image, as the bar chart and beeswarm plot confidence intervals
        do not overlap, then we can say that there is a statistically
        significant difference between the absolute errors between the bar chart
        task and the beeswarm plot task. Additionally, as the bar chart task
        gave us a wider confidence interval, it also suggests that there was a
        higher variance of errors for that task than the beeswarm task.
        Moreover, the fact that the bar chart task gave us a wider confidence
        interval than the beeswarm plot task also suggests that the bar chart
        task was the harder of the two tasks for users. Users tended to more
        drastically underestimate or overestimate the x-coordinate of the median
        for the bar chart task in comparison to the beeswarm plot task. As the
        bar chart and density plot confidence intervals do not overlap, then we
        can say that there is a statistically significant difference between the
        absolute errors between the bar chart task and the density plot task.
        Additionally, as the density plot task gave us a wider confidence
        interval, it also suggests that there was a higher variance of errors
        for that task than the bar chart task. Moreover, the fact that the
        density plot task gave us a wider confidence interval than the bar chart
        task also suggests that the density plot task was the harder of the two
        tasks for users. Users tended to more drastically underestimate or
        overestimate the x-coordinate of the median for the density plot task in
        comparison to the bar chart task. As the scatter plot and beeswarm plot
        confidence intervals do not overlap, then we can say that there is a
        statistically significant difference between the absolute errors between
        the bar scatter plot and the beeswarm plot task. Additionally, as the
        scatter plot task gave us a wider confidence interval, it also suggests
        that there was a higher variance of errors for that task than the
        beeswarm plot task. Moreover, the fact that the scatter plot task gave
        us a wider confidence interval than the beeswarm plot task also suggests
        that the scatter plot task was the harder of the two tasks for users.
        Users tended to more drastically underestimate or overestimate the
        x-coordinate of the median for the scatter plot task in comparison to
        the beeswarm plot task. As the scatter plot and density plot confidence
        intervals do not overlap, then we can say that there is a statistically
        significant difference between the absolute errors between the scatter
        plot and the density plot task. Additionally, as the scatter plot task
        gave us a wider confidence interval, it also suggests that there was a
        higher variance of errors for that task than the density plot task.
        Moreover, the fact that the scatter plot task gave us a wider confidence
        interval than the density plot task also suggests that the scatter plot
        task was the harder of the two tasks for users. Users tended to more
        drastically underestimate or overestimate the x-coordinate of the median
        for the scatter plot task in comparison to the density plot task. As the
        scatter plot and histogram confidence intervals do not overlap, then we
        can say that there is a statistically significant difference between the
        absolute errors of the scatter plot and histogram tasks. Additionally,
        as the scatter plot task gave us a wider confidence interval, it also
        suggests that there was a higher variance of errors for that task than
        the histogram task. Moreover, the fact that the scatter plot task gave
        us a wider confidence interval than the histogram task also suggests
        that the scatter plot task was the harder of the two tasks for users.
        Users tended to more drastically underestimate or overestimate the
        x-coordinate of the median for the scatter plot task in comparison to
        the histogram task.
      </p>
      <h1>Example Visualizations</h1>
      Below are actual examples of each of the different visualizations used in
      this experiment:
      <h2>Histogram</h2>
      <img src="histogram.png" alt="Histogram" height="700" width="1000" />
      <h2>Density Plot</h2>
      <img
        src="density_plot.png"
        alt="Density Plot"
        height="700"
        width="1000"
      />
      <h2>Scatter Plot</h2>
      <img
        src="scatter_plot.png"
        alt="Scatter Plot"
        height="700"
        width="1000"
      />
      <h2>Beeswarm Plot</h2>
      <img
        src="beeswarm_plot.png"
        alt="Beeswarm Plot"
        height="700"
        width="1000"
      />
      <h2>Bar Chart</h2>
      <img src="bar_chart.png" alt="Bar Chart" height="700" width="1000" />
    </div>
  </body>
</html>
